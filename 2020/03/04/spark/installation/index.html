<!DOCTYPE html><html lang="zh-cn"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><title>Spark 3.0 集群搭建教程</title>
<link rel="stylesheet" href="/css/layout.css">
<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/css/vendors/rabbit-lyrics.css">
<link rel="shortcut icon" href="/img/favicon.ico"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="简单易懂の现代魔法" type="application/atom+xml">
</head><body><header class="base-menu"><div class="menu-wrapper"><div class="menu-layout-bg"></div><nav class="menu-content"><ul class="menu-list"><li class="menu-item"><a class="menu-link" href="/">简单易懂の现代魔法</a></li><li class="menu-item"><a class="menu-link" href="/archives/">Archives</a></li><li class="menu-item"><a class="menu-link" target="_blank" rel="noopener" href="http://weibo.com/u/2360401155">Weibo</a></li><li class="menu-item"><a class="menu-link" href="/about">About</a></li><li class="menu-item"><a class="menu-link" href="/atom.xml">Rss</a></li></ul></nav></div></header><header class="base-mobile-menu"><div class="mobile-menu-wrapper"><div class="menu-layout-bg"></div><nav class="menu-content"><ul class="menu-list"><li class="menu-item"><a class="menu-link" href="/">简单易懂の现代魔法</a></li><li class="menu-item"><a class="menu-link" href="/archives/">Archives</a></li><li class="menu-item"><a class="menu-link" target="_blank" rel="noopener" href="http://weibo.com/u/2360401155">Weibo</a></li><li class="menu-item"><a class="menu-link" href="/about">About</a></li><li class="menu-item"><a class="menu-link" href="/atom.xml">Rss</a></li></ul></nav><div class="toggle-menu" id="mobile-menu-toggle"><span class="menu-bar"></span><span class="menu-bar"> </span><span class="menu-bar"></span></div></div></header><div class="base-content"><div class="base-content-main"><article class="article-main"><h1 class="article-title">Spark 3.0 集群搭建教程</h1><div class="article-meta"><p class="meta-item meta-time"><span class="meta-item-title"></span><i class="icon-calendar"> </i>2020-03-04</p><p class="meta-item meta-tag"><span class="meta-item-title"></span><i class="icon-tag"> </i><a class="tag-link" href="/tags/spark/">spark</a></p></div><div class="article-content"><span id="more"></span>
<h2 id="先决条件"><a class="header-anchor" href="#先决条件">¶</a>先决条件</h2>
<p>在部署Spark之前，请确认集群的每个节点都符合以下条件：</p>
<ol>
<li>已安装Java 1.8.x或以上版本（推荐1.8版本）</li>
<li>节点两两之间可以SSH免密码登录</li>
<li>已部署Hadoop（如果只是部署Standalone Cluster则不需要Hadoop）</li>
</ol>
<p>如果你已经按照<a href="/2020/03/04/hadoop/installation">Hadoop集群搭建教程</a>成功建立了Hadoop集群，那么以上条件均已满足。</p>
<h3 id="下载Spark二进制文件"><a class="header-anchor" href="#下载Spark二进制文件">¶</a>下载Spark二进制文件</h3>
<p>在Spark的<a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">下载页面</a>中有多个版本可以选择，因为之前选择了Hadoop 2.7.7版本，所以这里选择与之对应的 <a target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0/spark-3.0.0-bin-without-hadoop.tgz">Pre-built with user-provided Apache Hadoop</a>版本，Scala版本选择最新的2.12。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.0.0/spark-3.0.0-bin-without-hadoop.tgz</span><br><span class="line">tar -xzvf spark-3.0.0-bin-without-hadoop.tgz</span><br></pre></td></tr></table></figure>
<h3 id="配置Spark环境变量"><a class="header-anchor" href="#配置Spark环境变量">¶</a>配置Spark环境变量</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Spark environment</span></span><br><span class="line">export SPARK_HOME=/opt/spark-3.0.0-bin-without-hadoop</span><br><span class="line">export PATH=$PATH:$&#123;SPARK_HOME&#125;/bin</span><br></pre></td></tr></table></figure>
<h4 id="spark-env-sh"><a class="header-anchor" href="#spark-env-sh">¶</a><a target="_blank" rel="noopener" href="http://spark-env.sh">spark-env.sh</a></h4>
<p>因为我们下载的是不带hadoop依赖jar的spark版本，所以需要在spark中指定hadoop的classpath</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">配置文件spark-env.sh：</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">## in conf/spark-env.sh ###</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">If <span class="string">&#x27;hadoop&#x27;</span> binary is on your PATH</span></span><br><span class="line">export SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">With explicit path to <span class="string">&#x27;hadoop&#x27;</span> binary</span></span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Passing a Hadoop configuration directory</span></span><br><span class="line">export SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)</span><br></pre></td></tr></table></figure>
<p>这里在最后一行添加：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/opt/hadoop-2.7.7/bin/hadoop classpath)</span><br></pre></td></tr></table></figure>
<h3 id="slaves"><a class="header-anchor" href="#slaves">¶</a>slaves</h3>
<p>向slaves文件写入slave节点的host/IP地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">huawei-02</span><br><span class="line">huawei-03</span><br></pre></td></tr></table></figure>
<h3 id="将配置好的Spark分发到其它节点"><a class="header-anchor" href="#将配置好的Spark分发到其它节点">¶</a>将配置好的Spark分发到其它节点</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/spark-3.0.0-bin-without-hadoop huawei-02:/opt/spark-3.0.0-bin-without-hadoop</span><br><span class="line">scp -r /opt/spark-3.0.0-bin-without-hadoop huawei-03:/opt/spark-3.0.0-bin-without-hadoop</span><br></pre></td></tr></table></figure>
<h3 id="启动Spark"><a class="header-anchor" href="#启动Spark">¶</a>启动Spark</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./opt/spark-3.0.0=bin-without-hadoop/sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>打开浏览器，输入<code>[主机]:8080</code> 出现下面节点则表示启动成功了。</p>
<p><img src="/img/spark/spark-master.png" alt="spark cluster"></p>
<p>关于：<code>WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code>问题的解决。</p>
<p>其实该问题不影响使用。就是没有加载到linux共享库，打开<code>/etc/rpfile</code>，</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>
<p>source一下就解决了。</p>
<p>查看<code>sbin</code>目录，可以看到spark提供了几种启动方式.</p>
<h3 id="Spark-的三种启动方式"><a class="header-anchor" href="#Spark-的三种启动方式">¶</a>Spark 的三种启动方式</h3>
<p>Spark最主要资源管理方式按排名为Hadoop Yarn, Apache Standalone 和Mesos。在单机使用时，Spark还可以采用最基本的local模式。</p>
<p>目前Apache Spark支持三种分布式部署方式，分别是standalone、spark on mesos和 spark on YARN，其中，第一种类似于MapReduce 1.0所采用的模式，内部实现了容错性和资源管理，后两种则是未来发展的趋势，部分容错性和资源管理交由统一的资源管理系统完成：让Spark运行在一个通用的资源管理系统之上，这样可以与其他计算框架，比如MapReduce，公用一个集群资源，最大的好处是降低运维成本和提高资源利用率（资源按需分配）。本文将介绍这三种部署方式，并比较其优缺点。</p>
<ol>
<li>Standalone模式</li>
</ol>
<p>即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。借鉴Spark开发模式，我们可以得到一种开发新型计算框架的一般思路：先设计出它的standalone模式，为了快速开发，起初不需要考虑服务（比如master/slave）的容错性，之后再开发相应的wrapper，将stanlone模式下的服务原封不动的部署到资源管理系统yarn或者mesos上，由资源管理系统负责服务本身的容错。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的：</p>
<ol>
<li>都是由master/slaves服务组成的，且起初master均存在单点故障，后来均通过zookeeper解决（Apache MRv1的JobTracker仍存在单点问题，但CDH版本得到了解决）；</li>
<li>各个节点上的资源被抽象成粗粒度的slot，有多少slot就能同时运行多少task。不同的是，MapReduce将slot分为map slot和reduce slot，它们分别只能供Map Task和Reduce Task使用，而不能共享，这是MapReduce资源利率低效的原因之一，而Spark则更优化一些，它不区分slot类型，只有一种slot，可以供各种类型的Task使用，这种方式可以提高资源利用率，但是不够灵活，不能为不同类型的Task定制slot资源。总之，这两种方式各有优缺点。</li>
</ol>
<ol start="2">
<li>Spark On Mesos模式</li>
</ol>
<p>这是很多公司采用的模式，官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。目前在Spark On Mesos环境中，用户可选择两种调度模式之一运行自己的应用程序（可参考Andrew Xia的“Mesos Scheduling Mode on Spark”）：</p>
<ol>
<li>
<p>粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。举个例子，比如你提交应用程序时，指定使用5个executor运行你的应用程序，每个executor占用5GB内存和5个CPU，每个executor内部设置了5个slot，则Mesos需要先为executor分配资源并启动它们，之后开始调度任务。另外，在程序运行过程中，mesos的master和slave并不知道executor内部各个task的运行情况，executor直接将任务状态通过内部的通信机制汇报给Driver，从一定程度上可以认为，每个应用程序利用mesos搭建了一个虚拟集群自己使用。</p>
</li>
<li>
<p>细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。每个Task会汇报状态给Mesos slave和Mesos Master，便于更加细粒度管理和容错，这种调度模式类似于MapReduce调度模式，每个Task完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。</p>
</li>
</ol>
<ol start="3">
<li>Spark On YARN模式</li>
</ol>
<p>这是一种很有前景的部署模式。但限于YARN自身的发展，目前仅支持粗粒度模式（Coarse-grained Mode）。这是由于YARN上的Container资源是不可以动态伸缩的，一旦Container启动之后，可使用的资源不能再发生变化，不过这个已经在YARN计划中了。<br>
spark on yarn 的支持两种模式：</p>
<ol>
<li>yarn-cluster：适用于生产环境；</li>
<li>yarn-client：适用于交互、调试，希望立即看到app的输出</li>
</ol>
<p>yarn-cluster和yarn-client的区别在于yarn appMaster，每个yarn app实例有一个appMaster进程，是为app启动的第一个container；负责从ResourceManager请求资源，获取到资源后，告诉NodeManager为其启动container。yarn-cluster和yarn-client模式内部实现还是有很大的区别。如果你需要用于生产环境，那么请选择yarn-cluster；而如果你仅仅是Debug程序，可以选择yarn-client。</p>
<p>总结：<br>
这三种分布式部署方式各有利弊，通常需要根据实际情况决定采用哪种方案。进行方案选择时，往往要考虑公司的技术路线（采用Hadoop生态系统还是其他生态系统）、相关技术人才储备等。上面涉及到Spark的许多部署模式，究竟哪种模式好这个很难说，需要根据你的需求，如果你只是测试Spark Application，你可以选择local模式。而如果你数据量不是很多，Standalone 是个不错的选择。当你需要统一管理集群资源（Hadoop、Spark等），那么你可以选择Yarn或者mesos，但是这样维护成本就会变高。<br>
· 从对比上看，mesos似乎是Spark更好的选择，也是被官方推荐的<br>
· 但如果你同时运行hadoop和Spark,从兼容性上考虑，Yarn是更好的选择。 · 如果你不仅运行了hadoop，spark。还在资源管理上运行了docker，Mesos更加通用。<br>
· Standalone对于小规模计算集群更适合！</p>
<p>更多关于启动模式的介绍，可以查看<a href="/2018/10/26/spark/spark-cluster-config">先前</a>的文章。</p>
</div></article><nav class="article-nav"><div class="article-nav-prev"><a href="/2020/03/11/learning-plan/2020_03_11_study_plan/">2020年学习计划</a></div><div class="article-nav-next"><a href="/2020/03/04/flink/installation/">Flink 集群搭建教程</a></div></nav><div id="base-discus"><div id="disqus_thread"></div><script>var disqus_shortname = 'barudisshu-github-io';
var disqus_identifier = '2020/03/04/spark/installation/';
var disqus_title = 'Spark 3.0 集群搭建教程';
var disqus_url = 'https://galudisu.info/2020/03/04/spark/installation/';
(function () {
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async>  </script></div></div></div><footer class="base-footer"><div class="footer-wrapper"><span>©2016 - 2025 <a href="https://galudisu.info">barudisshu</a>, unless otherwise noted.</span></div></footer><div class="dom-ready">
<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>

<link rel="stylesheet" href="//cdn.bootcss.com/pace/1.0.2/themes/green/pace-theme-flash.min.css">

<script src="/js/base.js"></script>

<script src="/js/rabbit-lyrics.js"></script>
</div></body></html>