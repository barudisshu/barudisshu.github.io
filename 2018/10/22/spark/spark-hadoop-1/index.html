<!DOCTYPE html><html lang="zh-cn"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><title>Hadoop集群配置</title>
<link rel="stylesheet" href="/css/layout.css">
<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/css/vendors/rabbit-lyrics.css">
<link rel="shortcut icon" href="/img/favicon.ico"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="简单易懂の现代魔法" type="application/atom+xml">
</head><body><header class="base-menu"><div class="menu-wrapper"><div class="menu-layout-bg"></div><nav class="menu-content"><ul class="menu-list"><li class="menu-item"><a class="menu-link" href="/">简单易懂の现代魔法</a></li><li class="menu-item"><a class="menu-link" href="/maths/">Maths</a></li><li class="menu-item"><a class="menu-link" href="/projects/">Projects</a></li><li class="menu-item"><a class="menu-link" href="/archives/">Archives</a></li><li class="menu-item"><a class="menu-link" target="_blank" rel="noopener" href="http://weibo.com/u/2360401155">Weibo</a></li><li class="menu-item"><a class="menu-link" href="/about">About</a></li><li class="menu-item"><a class="menu-link" href="/atom.xml">Rss</a></li></ul></nav></div></header><header class="base-mobile-menu"><div class="mobile-menu-wrapper"><div class="menu-layout-bg"></div><nav class="menu-content"><ul class="menu-list"><li class="menu-item"><a class="menu-link" href="/">简单易懂の现代魔法</a></li><li class="menu-item"><a class="menu-link" href="/maths/">Maths</a></li><li class="menu-item"><a class="menu-link" href="/projects/">Projects</a></li><li class="menu-item"><a class="menu-link" href="/archives/">Archives</a></li><li class="menu-item"><a class="menu-link" target="_blank" rel="noopener" href="http://weibo.com/u/2360401155">Weibo</a></li><li class="menu-item"><a class="menu-link" href="/about">About</a></li><li class="menu-item"><a class="menu-link" href="/atom.xml">Rss</a></li></ul></nav><div class="toggle-menu" id="mobile-menu-toggle"><span class="menu-bar"></span><span class="menu-bar"> </span><span class="menu-bar"></span></div></div></header><div class="base-content"><div class="base-content-main"><article class="article-main"><h1 class="article-title">Hadoop集群配置</h1><div class="article-meta"><p class="meta-item meta-time"><span class="meta-item-title"></span><i class="icon-calendar"> </i>2018-10-22</p><p class="meta-item meta-tag"><span class="meta-item-title"></span><i class="icon-tag"> </i><a class="tag-link" href="/tags/hadoop/">hadoop</a></p></div><div class="article-content"><span id="more"></span>
<h2 id="完全分布式集群-HA"><a class="header-anchor" href="#完全分布式集群-HA">¶</a>完全分布式集群(HA)</h2>
<ol>
<li>环境准备</li>
</ol>
<ul>
<li>修改IP</li>
<li>修改主机名及主机名和IP地址的映射</li>
<li>关闭防火墙</li>
<li>ssh免密登录</li>
<li>安装JDK，配置环境变量</li>
</ul>
<ol start="2">
<li>集群规则</li>
</ol>
<table>
<thead>
<tr>
<th>节点名称</th>
<th>NN</th>
<th>JJN</th>
<th>DN</th>
<th>ZK/FC</th>
<th>ZK</th>
<th>RM</th>
<th>NM</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark-node1</td>
<td>NameNode</td>
<td>JournalNode</td>
<td>DataNode</td>
<td>ZK/FC</td>
<td>ZooKeeper</td>
<td></td>
<td>NodeManager</td>
</tr>
<tr>
<td>spark-node2</td>
<td>NameNode</td>
<td>JournalNode</td>
<td>DataNode</td>
<td>ZK/FC</td>
<td>ZooKeeper</td>
<td>ResourceManager</td>
<td>NodeManager</td>
</tr>
<tr>
<td>spark-node3</td>
<td></td>
<td>JournalNode</td>
<td>DataNode</td>
<td></td>
<td>ZooKeeper</td>
<td>ResourceManager</td>
<td>NodeManager</td>
</tr>
</tbody>
</table>
<ol start="3">
<li>安装Zookeeper集群</li>
</ol>
<p>安装详解参考：<a href="http://galudisu.info/2018/10/21/spark/spark-zookeeper-1/">CentOS 7.5 搭建Zookeeper集群与命令行操作</a></p>
<ol start="4">
<li>设置SSH免密钥</li>
</ol>
<p>关于ssh免密钥的设置，要求每两台主机之前设置免密码，自己的主机与自己的主机之间也要设置免密码。这项操作可以在admin用户下执行，执行完毕公钥在<code>/home/xxx/.ssh/id_rsa.pub</code>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]# ssh-keygen -t rsa</span><br><span class="line">[xxx@spark-node1 ~]# ssh-copy-id spark-node1</span><br><span class="line">[xxx@spark-node1 ~]# ssh-copy-id spark-node2</span><br><span class="line">[xxx@spark-node1 ~]# ssh-copy-id spark-node3</span><br></pre></td></tr></table></figure>
<p>spark-node1与spark-node2为namenode节点要相互免密钥 HDFS的HA</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node2 ~]# ssh-keygen -t rsa</span><br><span class="line">[xxx@spark-node2 ~]# ssh-copy-id spark-node2</span><br><span class="line">[xxx@spark-node2 ~]# ssh-copy-id spark-node1</span><br><span class="line">[xxx@spark-node2 ~]# ssh-copy-id spark-node3</span><br></pre></td></tr></table></figure>
<p>spark-node2与spark-node3为yarn节点要相互免密钥 YARN的HA</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node3 ~]# ssh-keygen -t rsa</span><br><span class="line">[xxx@spark-node3 ~]# ssh-copy-id spark-node3</span><br><span class="line">[xxx@spark-node3 ~]# ssh-copy-id spark-node1</span><br><span class="line">[xxx@spark-node3 ~]# ssh-copy-id spark-node2</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>安装配置Hadoop集群</li>
</ol>
<p>解压<code>hadoop-2.7.6.tar.gz</code> 到 <code>/opt/</code>目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar zxvf hadoop-2.7.6.tar.gz -C /opt/</span><br></pre></td></tr></table></figure>
<p>创建软链接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /opt/hadoop-2.7.6 /opt/hadoop</span><br></pre></td></tr></table></figure>
<p>配置Hadoop集群，配置文件都在<code>/opt/hadoop/etc/hadoop/</code>下，修改<code>hadoop-env.sh</code>，<code>mapred-env.sh</code>，<code>yarn-env.sh</code>的JAVA环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_191-amd64</span><br></pre></td></tr></table></figure>
<p>修改core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 把两个NameNode的地址组装成一个集群mycluster --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/data/ha/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定ZKFC故障自动切换转移 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node1:2181,spark-node2:2181,spark-node3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置dfs副本数，默认3个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 完全分布式集群名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node2:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node2:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://spark-node1:8485;spark-node2:8485;spark-node3:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/admin/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 声明journalnode服务器存储目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/data/ha/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 关闭权限检查--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置自动故障转移--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>　</span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改mapred-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定mr框架为yarn方式 --&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定mr历史服务器主机,端口 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定mr历史服务器WebUI主机,端口 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-node1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器的WEB UI上最多显示20000个历史的作业记录信息 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.joblist.cache.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--配置作业运行日志 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;yarn.app.mapreduce.am.staging-dir&#125;/history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;yarn.app.mapreduce.am.staging-dir&#125;/history/done_intermediate<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.staging-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/hadoop-yarn/staging<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改slaves</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br><span class="line">spark-node1</span><br><span class="line">spark-node2</span><br><span class="line">spark-node3</span><br></pre></td></tr></table></figure>
<p>修改yarn-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- reducer获取数据的方式 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!--启用resourcemanager ha--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!--声明两台resourcemanager的地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;rmCluster&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;spark-node2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;spark-node3&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!--指定zookeeper集群的地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;spark-node1:2181,spark-node2:2181,spark-node3:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!--启用自动恢复--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>拷贝hadoop到其它节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r hadoop-2.7.6/ xxx@spark-node2:/opt/</span><br><span class="line">scp -r hadoop-2.7.6/ xxx@spark-node3:/opt/</span><br></pre></td></tr></table></figure>
<p>配置Hadoop环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 末尾追加</span></span></span><br><span class="line">export  HADOOP_HOME=/opt/hadoop-2.7.6</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>启动集群</li>
</ol>
<ol>
<li>在各个JournalNode节点上，输入以下命令启动journalnode服务：(前提zookeeper集群已启动)</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ hadoop-daemon.sh start journalnode</span><br><span class="line">[xxx@spark-node2 ~]$ hadoop-daemon.sh start journalnode</span><br><span class="line">[xxx@spark-node3 ~]$ hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<p>启动Journalnode是为了创建<code>/data/ha/jn</code>，此时jn里面是空的</p>
<ol start="2">
<li>在<code>[nn1 ]</code>上，对namenode进行格式化，并启动：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p><img src="/img/spark/hadoop-format.png" alt="hadoop-format"></p>
<p>格式化namenode，此时jn里面会产生集群ID等信息</p>
<p><img src="/img/spark/hadoop-id.png" alt="hadoop-id"></p>
<p>另外，<code>/data/ha/tmp</code>也会产生如下信息</p>
<p><img src="/img/spark/hadoop-tmp.png" alt="hadoop-tmp"></p>
<p>启动nn1上namenode</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 current]$ hadoop-daemon.sh  start namenode</span><br><span class="line">starting namenode, logging to /opt/hadoop-2.7.6/logs/hadoop-admin-namenode-node21.out</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>在<code>[nn2]</code>上，同步nn1的元数据信息：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node2 ~]$ hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
<p><img src="/img/spark/hadoop-nn2.png" alt="hadoop-nn2"></p>
<ol start="4">
<li>启动<code>[nn2]</code>：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node2 ~]$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>在<code>[nn1]</code>上，启动所有datanode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ hadoop-daemons.sh start datanode</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>查看web页面此时显示</li>
</ol>
<p><img src="/img/spark/hadoop-node1.png" alt="hadoop-node1"><br>
<img src="/img/spark/hadoop-node2.png" alt="hadoop-node2"></p>
<ol start="7">
<li>手动切换状态，在各个namenode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的namenode就是Active NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ hadoop-daemon.sh start zkfc</span><br><span class="line">[xxx@spark-node2 ~]$ hadoop-daemon.sh start zkfc</span><br></pre></td></tr></table></figure>
<p>或者强制手动其中一个节点变为Active</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node2 data]$ hdfs haadmin -transitionToActive nn1 --forcemanual</span><br></pre></td></tr></table></figure>
<p>web页面查看</p>
<p><img src="/img/spark/hadoop-active.png" alt="hadoop-active"></p>
<ol start="8">
<li>自动切换状态，需要初始化HA在zookeeper中状态，先停掉hdfs服务，然后随便找一台zookeeper的安装节点</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 current]$ hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>
<p>查看，此时会产生一个hadoop-ha目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@spark-node2 ~]# zkCli.sh</span><br></pre></td></tr></table></figure>
<p>启动hdfs服务，查看namenode状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ start-dfs.sh</span><br></pre></td></tr></table></figure>
<ol start="9">
<li>验证</li>
</ol>
<ul>
<li>将Active NameNode进程kill</li>
<li>将Active NameNode机器断开网络</li>
</ul>
<ol start="10">
<li>启动yarn</li>
</ol>
<p>在spark-node2中执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node2 ~]$ start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>在spark-node3中执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node3 ~]$ yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>
<p>查看服务状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node2 ~]$ yarn rmadmin -getServiceState rm1</span><br><span class="line">active</span><br><span class="line">[xxx@spark-node2 ~]$ yarn rmadmin -getServiceState rm2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure>
<p><img src="/img/spark/hadoop-yarn.png" alt="hadoop-yarn"></p>
<ol start="6">
<li>测试集群</li>
</ol>
<ol>
<li>查看进程</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ start-dfs.sh </span><br><span class="line">[xxx@spark-node2 ~]$ start-yarn.sh </span><br><span class="line">[xxx@spark-node3 ~]$ yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxx @ spark-node1 <span class="keyword">in</span> ~ [10:29:20]</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">jps</span></span><br><span class="line">3220 JournalNode</span><br><span class="line">3588 DataNode</span><br><span class="line">4967 Jps</span><br><span class="line">4378 NodeManager</span><br><span class="line">3725 DFSZKFailoverController</span><br><span class="line">3407 NameNode</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxx @ spark-node2 <span class="keyword">in</span> ~ [10:28:32]</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">jps</span>          </span><br><span class="line">3939 ResourceManager</span><br><span class="line">3380 NameNode</span><br><span class="line">3508 DataNode</span><br><span class="line">4040 NodeManager</span><br><span class="line">3660 DFSZKFailoverController</span><br><span class="line">4621 Jps</span><br><span class="line">3182 JournalNode</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxx @ spark-node3 <span class="keyword">in</span> ~ [10:28:09]</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">jps</span>          </span><br><span class="line">3188 JournalNode</span><br><span class="line">3989 Jps</span><br><span class="line">3784 ResourceManager</span><br><span class="line">3641 NodeManager</span><br><span class="line">3371 DataNode</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>任务提交</li>
</ol>
<p>上传文件到集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ hadoop fs -mkdir -p /user/galudisu/input</span><br><span class="line">[xxx@spark-node1 ~]$ mkdir -p  /opt/wcinput/</span><br><span class="line">[xxx@spark-node1 ~]$ vi  /opt/wcinput/wc.txt </span><br><span class="line">[xxx@spark-node1 ~]$ hadoop fs -put  /opt/wcinput/wc.txt /user/galudisu/input</span><br></pre></td></tr></table></figure>
<p>wc.txt文本内容为</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop spark   storm</span><br><span class="line">hbase hive sqoop</span><br><span class="line">hadoop flink flume</span><br><span class="line">spark hadoop</span><br></pre></td></tr></table></figure>
<p>上传文件后查看文件存放在什么位置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">文件存储路径</span><br><span class="line">[xxx@spark-node1 subdir0]$ pwd</span><br><span class="line">/opt/hadoop/data/ha/tmp/dfs/data/current/BP-1244373306-192.168.100.21-1527653416622/current/finalized/subdir0/subdir0</span><br><span class="line">查看文件内容</span><br><span class="line">[xxx@spark-node1 subdir0]$ cat blk_1073741825</span><br><span class="line">hadoop spark   storm</span><br><span class="line">hbase hive sqoop</span><br><span class="line">hadoop flink flume</span><br><span class="line">spark hadoop</span><br></pre></td></tr></table></figure>
<p>下载文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 opt]$ hadoop fs -get /user/admin/input/wc.txt</span><br></pre></td></tr></table></figure>
<p>执行wordcount程序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xxx@spark-node1 ~]$ hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount /user/galudisu/input /user/galudisu/output</span><br></pre></td></tr></table></figure>
<p>执行过程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">18/10/22 10:44:06 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">18/10/22 10:44:06 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">18/10/22 10:44:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540218481613_0001</span><br><span class="line">18/10/22 10:44:07 INFO impl.YarnClientImpl: Submitted application application_1540218481613_0001</span><br><span class="line">18/10/22 10:44:07 INFO mapreduce.Job: The url to track the job: http://spark-node2:8088/proxy/application_1540218481613_0001/</span><br><span class="line">18/10/22 10:44:07 INFO mapreduce.Job: Running job: job_1540218481613_0001</span><br><span class="line">18/10/22 10:44:19 INFO mapreduce.Job: Job job_1540218481613_0001 running in uber mode : false</span><br><span class="line">18/10/22 10:44:19 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/10/22 10:44:31 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/10/22 10:44:39 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">18/10/22 10:44:40 INFO mapreduce.Job: Job job_1540218481613_0001 completed successfully</span><br><span class="line">18/10/22 10:44:40 INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=102</span><br><span class="line">		FILE: Number of bytes written=250893</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=178</span><br><span class="line">		HDFS: Number of bytes written=64</span><br><span class="line">		HDFS: Number of read operations=6</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=9932</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=4401</span><br><span class="line">		Total time spent by all map tasks (ms)=9932</span><br><span class="line">		Total time spent by all reduce tasks (ms)=4401</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=9932</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=4401</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=10170368</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=4506624</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=4</span><br><span class="line">		Map output records=11</span><br><span class="line">		Map output bytes=112</span><br><span class="line">		Map output materialized bytes=102</span><br><span class="line">		Input split bytes=108</span><br><span class="line">		Combine input records=11</span><br><span class="line">		Combine output records=8</span><br><span class="line">		Reduce input groups=8</span><br><span class="line">		Reduce shuffle bytes=102</span><br><span class="line">		Reduce input records=8</span><br><span class="line">		Reduce output records=8</span><br><span class="line">		Spilled Records=16</span><br><span class="line">		Shuffled Maps =1</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=1</span><br><span class="line">		GC time elapsed (ms)=186</span><br><span class="line">		CPU time spent (ms)=1950</span><br><span class="line">		Physical memory (bytes) snapshot=291643392</span><br><span class="line">		Virtual memory (bytes) snapshot=4170993664</span><br><span class="line">		Total committed heap usage (bytes)=141291520</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=70</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=64</span><br></pre></td></tr></table></figure>
<p>下载查看</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxx @ spark-node1 <span class="keyword">in</span> ~ [10:52:01]</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">hadoop fs -get /user/galudisu/output/part-r-00000</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">galudisu @ spark-node1 <span class="keyword">in</span> ~ [10:52:06]</span> </span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> part-r-00000</span> </span><br><span class="line">flink	1</span><br><span class="line">flume	1</span><br><span class="line">hadoop	3</span><br><span class="line">hbase	1</span><br><span class="line">hive	1</span><br><span class="line">spark	2</span><br><span class="line">sqoop	1</span><br><span class="line">storm	1</span><br></pre></td></tr></table></figure>
<h2 id="Hadoop加入Systemd服务"><a class="header-anchor" href="#Hadoop加入Systemd服务">¶</a>Hadoop加入Systemd服务</h2>
<p>类似于ZooKeeper，将hadoop加入systemd中，让系统启动后自启</p>
<p>首先，在各个节点的hadoop目录创建一个启动脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">start() &#123;</span><br><span class="line">    source &quot;/etc/profile&quot;</span><br><span class="line"></span><br><span class="line">    /opt/hadoop/sbin/start-dfs.sh</span><br><span class="line">    /opt/hadoop/sbin/start-yarn.sh</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop() &#123;</span><br><span class="line">    source &quot;/etc/profile&quot;</span><br><span class="line"></span><br><span class="line">    /opt/hadoop/sbin/stop-yarn.sh</span><br><span class="line">    /opt/hadoop/sbin/stop-dfs.sh</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start|stop) &quot;$1&quot; ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure>
<p>加入系统Systemd开机启动，注意下面after，没有的服务要去掉</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Hadoop DFS namenode and datanode</span><br><span class="line">After=syslog.target network.target remote-fs.target nss-lookup.target network-online.target</span><br><span class="line">Requires=network-online.target</span><br><span class="line">Wants=zookeeper.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=spark</span><br><span class="line">Group=spark</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/opt/hadoop/hadoop-service.sh start</span><br><span class="line">ExecStop=/opt/hadoop/hadoop-service.sh stop</span><br><span class="line">RemainAfterExit=yes</span><br><span class="line">Environment=JAVA_HOME=/usr/java/jdk1.8.0_191-amd64</span><br><span class="line">Environment=HADOOP_HOME=/opt/hadoop</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>
<p>按照上面的方式，单独修改每个节点的<code>hadoop-service.sh</code>即可。</p>
<p><img src="/img/spark/hadoop-data-node.png" alt="hadoop-data-node"></p>
</div></article><nav class="article-nav"><div class="article-nav-prev"><a href="/2018/10/26/spark/spark-cluster-config/">CentOS 7 搭建Spark2.3.1分布式集群</a></div><div class="article-nav-next"><a href="/2018/10/21/spark/spark-zookeeper-2/">Zookeeper原理与API应用</a></div></nav><div id="base-discus"><div id="disqus_thread"></div><script>var disqus_shortname = 'barudisshu-github-io';
var disqus_identifier = '2018/10/22/spark/spark-hadoop-1/';
var disqus_title = 'Hadoop集群配置';
var disqus_url = 'https://galudisu.info/2018/10/22/spark/spark-hadoop-1/';
(function () {
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async>  </script></div></div></div><footer class="base-footer"><div class="footer-wrapper"><span>©2016 - 2025 <a href="https://galudisu.info">barudisshu</a>, unless otherwise noted.</span></div></footer><div class="dom-ready">
<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>

<link rel="stylesheet" href="//cdn.bootcss.com/pace/1.0.2/themes/green/pace-theme-flash.min.css">

<script src="/js/base.js"></script>

<script src="/js/rabbit-lyrics.js"></script>
</div></body></html>